{
    "project": "AI Troubleshooting Agent",
    "branchName": "ralph/gemini-support",
    "description": "Global support for Google Gemini models alongside OpenAI, with dynamic switching in the UI.",
    "userStories": [
        {
            "id": "US-001",
            "title": "Add Gemini Configuration & Env Support",
            "description": "As a developer, I need to configure the backend to accept Gemini API keys and model settings so that the application can authenticate with Google's services.",
            "acceptanceCriteria": [
                "Add `GOOGLE_API_KEY` to `.env` and `.env.example`",
                "Update configuration validation (Pydantic models) to allow `gemini` provider and model names",
                "Ensure default config still works if `GOOGLE_API_KEY` is missing (optional) or fail gracefully",
                "Typecheck passes"
            ],
            "priority": 1,
            "passes": false,
            "notes": ""
        },
        {
            "id": "US-002",
            "title": "Implement Gemini Model Provider",
            "description": "As a developer, I want a unified way to instantiate LLMs so that the code can switch between OpenAI and Gemini based on configuration.",
            "acceptanceCriteria": [
                "Create a factory or provider function that returns the correct LangChain chat model wrapper (`ChatOpenAI` or `ChatGoogleGenerativeAI`) based on input parameters",
                "Verify that the Gemini wrapper supports the necessary features (streaming, tool calling)",
                "Typecheck passes"
            ],
            "priority": 2,
            "passes": false,
            "notes": ""
        },
        {
            "id": "US-003",
            "title": "Update Backend to Accept Model Override",
            "description": "As a user, I want the backend to respect the model choice sent from the frontend request, overriding the default configuration.",
            "acceptanceCriteria": [
                "Update the API endpoint (e.g., `/chat`) to accept an optional `model` or `provider` parameter",
                "Pass this parameter down to the graph/agent initialization",
                "Ensure the correct model is used for the Orchestrator for that request",
                "Typecheck passes"
            ],
            "priority": 3,
            "passes": false,
            "notes": ""
        },
        {
            "id": "US-004",
            "title": "Frontend Model Selection UI",
            "description": "As a user, I want to select which AI model to use from the interface so that I can dynamically choose the best model for my task.",
            "acceptanceCriteria": [
                "Add a settings section (sidebar or expander) in Streamlit",
                "Add a dropdown/radio button to select between configured OpenAI and Gemini models",
                "The selection is preserved across chat messages in the session",
                "Verify in browser using dev-browser skill"
            ],
            "priority": 4,
            "passes": false,
            "notes": ""
        },
        {
            "id": "US-005",
            "title": "Global Model Propagation",
            "description": "As a user, when I select a model, I want it to apply to sub-agents as well (where applicable) to ensure consistent performance.",
            "acceptanceCriteria": [
                "Ensure the selected model is propagated to sub-agents initialized by the orchestrator",
                "Verify that sub-agents are actually using the requested model (e.g., via logs or behavior)",
                "Typecheck passes"
            ],
            "priority": 5,
            "passes": false,
            "notes": ""
        }
    ]
}